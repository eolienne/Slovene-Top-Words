{
 "metadata": {
  "name": "Top_Slovene_Words"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Top Slovene words\n\n## Using BeautifulSoup with Slovene\nScrapes a page for the [2000 most frequently used words](http://bos.zrc-sazu.si/a_top2000.html) across 7 different linguistic corpora for Slovene. Then it prints a random Slovene word from this list. \n* reassign the encoding setting for requests\n* read the text into BeautifulSoup\n* extract all of the tags without attributes into a set\n\n#### What it doesn't do: \nStemming/lemmatization of the words. Therefore some of the words are being presented with declensions that represent one of the three genders, seven cases, and/or three numbers (singular, dual, plural) in Slovene."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Setting up my packages\nimport requests, random\nfrom bs4 import BeautifulSoup",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Read in the page.  Get the table with data\ntop_2000 = requests.get('http://bos.zrc-sazu.si/a_top2000.html')\n\n#The header in the HTML page says that the text encoding is ISO-8859-2. Reassign. \ntop_2000.encoding = 'ISO-8859-2'\n\n#lxml is the recommended parser for BeautifulSoup\ntop_2000_page = BeautifulSoup(top_2000.text,\"lxml\")\ntop_2000_table = top_2000_page.find_all(\"table\")[1]\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Making a set of words from the BeautifulSoup parsed source page\nwords = set()\nfor td in top_2000_table.find_all(\"td\"):\n    if not td.attrs:\n        words.add(td.string.strip())\n    \n#Sorting the set by returning a list of the elements in a sorted order.\n#Special characters are put at the end\ntop_words = sorted(words)\n\n#Select random word from list\nprint(random.choice(top_words))\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "napak\n"
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "len(words)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": "5528"
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "I wanted to extract all of the words, but this page is structured with a bunch of <td> layers. To extract all words, they are in <td> but **not** in the <td> tags that have the align attribute. See below for a sample row. It goes across all columns (seven different linguistic corpora). I don't care which corpora they're from, for this project. Here is a sample of one row from the original file."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "<tr><td align=right>1.<td>   je<td align=right>34920<td>   je<td align=right>58338<td>   je<td align=right>35233<td>   je<td align=right>33210<td>   je<td align=right>33483<td>   je<td align=right>27892<td>   je<td align=right>32070<td align=right>   1.",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Errors\n\nAt first I saw a lot of: <code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb9 in position 164: invalid start byte </code>\n\nNot surprising, given that Slovene uses some special characters. \n\n### Here's what didn't work\n\n* <code>top_2000_page = BeautifulSoup(top_2000.text,\"lxml\", from_encoding = \"ISO-8859-2\")</code>\n* <code>top_2000_page = BeautifulSoup(top_2000.text,\"lxml\", from_encoding = \"latin-1\") </code>\n\nNeither did trying other [possible encodings](https://docs.python.org/3.4/library/codecs.html#encodings-and-unicode): <code>utf-8, utf-16, cp852, cp1250</code>. Let's be honest, I was just guessing what might fit Central European diacritics.\n\nSo I went **upstream** to the requesting the page. When I called <code>top_2000.text</code> the output looked...crazy. Not correct at all. \n\n### Here's what did work\n\nChecking the text encoding in the header. Yep. Where I should have started hours ago before getting into a much bigger rabbithole than what it seems here.\n\n<code><meta HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=ISO-8859-2\"></code>\n\nThen I tested the initial text from the request.\n<code>top_2000 = requests.get('http://bos.zrc-sazu.si/a_top2000.html')</code>\n<code>top_2000.encoding</code>\nIt claimed to be ISO-8859-1, but I had a suspicion Python was lying (Thanks [StackOverflow](http://stackoverflow.com/questions/27109725/python-and-beautifulsoup-encoding-issue-from-utf-8) for corroborating with me. \n\n#### The key to unlocking the beauty of proper encoding...\nLooking at the [Requests documentation for encoding](http://docs.python-requests.org/en/latest/user/quickstart/#make-a-request). Since I knew the text was jumbled before it even got into BeautifulSoup, Requests was the logical next step to check. So I reassigned the encoding.\n\n<code>top_2000.encoding = 'ISO-8859-2'</code>\n\nAfter reassigning the encoding, I ran the text through BeautifulSoup, as seen above, with lxml. \n\nThen the clouds parted, the sun emerged, and I felt victorious. Beautiful beautiful characters emerged. Dobrodo\u0161li, \u010d\u0161\u017e."
    }
   ],
   "metadata": {}
  }
 ]
}